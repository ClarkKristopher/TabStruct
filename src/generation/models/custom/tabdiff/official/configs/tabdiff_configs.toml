[data]
dequant_dist = "none"
int_dequant_factor = 0

[unimodmlp_params]
bias = true
d_token = 4
dim_t = 1024
factor = 32
n_head = 1
num_layers = 2
use_mlp = true

[diffusion_params]
cat_scheduler = 'log_linear' # 'log_linear', 'log_linear_unified', 'log_linear_per_column'
noise_dist = 'uniform_t' #'uniform_t' or 'log_norm'
num_timesteps = 50 
scheduler = 'power_mean' # 'power_mean', 'power_mean_unified', 'power_mean_per_column'

[diffusion_params.sampler_params]
second_order_correction = true
stochastic_sampler = true

[diffusion_params.edm_params]
net_conditioning = "sigma"
precond = true
sigma_data = 1.0

[diffusion_params.noise_dist_params]
P_mean = -1.2
P_std = 1.2

[diffusion_params.noise_schedule_params]
eps_max = 1e-3
eps_min = 1e-5
k_init = -6.0
k_offset = 1.0
rho = 7
rho_init = 7.0
rho_offset = 5.0
sigma_max = 80
sigma_min = 0.002

[train.main]
batch_size = 4096 
c_lambda = 1.0 
check_val_every = 30 
closs_weight_schedule = "anneal" 
d_lambda = 1.0 
ema_decay = 0.997 
factor = 0.90 # hyperparam for reduce_lr_on_plateau
lr = 0.001 
lr_scheduler = "reduce_lr_on_plateau" 
reduce_lr_patience = 50 # hyperparam for reduce_lr_on_plateau
steps = 300 
weight_decay = 0 

[sample]
batch_size = 10000
